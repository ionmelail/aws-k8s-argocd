name: Deploy EKS & ArgoCD

on:
  push:
    branches:
      - nodejs_nginx

jobs:
  deploy:
    runs-on: ubuntu-latest
    env:
      AWS_REGION: us-west-2
      CLUSTER_NAME: my-cluster
      POLICY_NAME: AmazonEKSLoadBalancerController
      SERVICE_ACCOUNT_NAMESPACE: kube-system
      SERVICE_ACCOUNT_NAME: aws-load-balancer-controller
    steps:
    
      # 1Ô∏è‚É£ Checkout Repository Code1211
      - name: Checkout Code
        uses: actions/checkout@v3

      # - name: Retrieve AWS Account ID
      #   id: aws_ac1coun1t1
      #   run: |111
      #     ACCOUNT_ID1=$(1aws sts ge1t-caller-identity1 --query "Account" --output text)
      #     echo "AWS_ACCOUNT_ID=$ACCOUNT_ID" >> $GITHUB_ENV

 
      #  Step 2: Configure AWS Credentials using GitHub Secrets
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2

   

      #  Step 3: Set up Terraform for Infrastructure Provisioning
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2

      # 4Ô∏è‚É£ Make Deployment Script Executable & Run It
      - name: Make Deployment Script Executable
        run: chmod +x scripts/Aws_K8s_Argocd.sh  

      - name: Run Deployment Script
        run: ./scripts/Aws_K8s_Argocd.sh  

      # 5Ô∏è‚É£ Initialize and Apply Terraform Configuration for EKS
      - name: Terraform Init & Apply
        run: |
          cd terraform
          terraform init
          terraform apply -auto-approve

      # 6Ô∏è‚É£ Update Kubeconfig to Access EKS Cluster
      - name: Update kubeconfig
        run: aws eks --region us-west-2 update-kubeconfig --name my-cluster

      # 7Ô∏è‚É£ Create ArgoCD Namespace if Not Exists
      - name: Create ArgoCD namespace
        run: |
          kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -

      # 8Ô∏è‚É£ Ensure ArgoCD Admin Secret Exists
      - name: Ensure ArgoCD Secret Exists
        run: |
          if ! kubectl get secret argocd-secret -n argocd; then
            kubectl create secret generic argocd-secret -n argocd --from-literal=admin.password='$2a$10$wEJ.NXBfjRj9JQ0QeqA1OuD4/2H6pRxH3p80fD/QFOhH8sD/jq12y'
          fi



      # 9Ô∏è‚É£ Install ArgoCD in the 'argocd' Namespace
      - name: Install ArgoCD
        run: |
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

       # 10. Install ArgoCD in the 'argocd' namespace
      - name: Wait for ArgoCD Pods to be Ready
        run: |
          echo "Waiting for all ArgoCD pods to be ready..."
          while [[ $(kubectl get pods -n argocd --no-headers | grep -c -v "Running") -ne 0 ]]; do
            echo "Some pods are still not ready..."
            kubectl get pods -n argocd
            kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

            sleep 10
          done
          echo "All ArgoCD pods are running!"

      # 11. Install ArgoCD in the 'argocd' namespace
      - name: Wait for All ArgoCD Pods to be Ready
        run: |
          echo "Waiting for all ArgoCD pods to be ready..."
          for i in {1..30}; do
            READY_PODS=$(kubectl get pods -n argocd --no-headers | awk '{print $2}' | grep -c "1/1")
            TOTAL_PODS=$(kubectl get pods -n argocd --no-headers | wc -l)
      
            if [[ "$READY_PODS" -eq "$TOTAL_PODS" ]]; then
              echo "‚úÖ All ArgoCD pods are ready!"
              exit 0
            fi
      
            echo "‚è≥ Waiting... $READY_PODS/$TOTAL_PODS pods are ready."
            sleep 10
          done
    
          echo "‚ùå Error: ArgoCD pods failed to reach 1/1 READY state."

      # ‚úÖ Install eksctl (Required for Load Balancer Controller)
      - name: Install eksctl
        run: |
          echo "Installing eksctl..."
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          eksctl version
          echo "‚úÖ eksctl installed successfully!"




      # üîπ Step 1: Ensure AWS Load Balancer Controller IAM Policy Exists
      - name: Ensure AWS Load Balancer Controller IAM Policy Exists
        run: |
          echo "üîç Checking AWS IAM Policy for LoadBalancer Controller..."
          POLICY_ARN=$(aws iam list-policies --query "Policies[?PolicyName=='AmazonEKSLoadBalancerController'].Arn" --output text)
          if [ -z "$POLICY_ARN" ]; then
            echo "‚ö†Ô∏è IAM Policy not found! Creating policy..."
            curl -s -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json
            aws iam create-policy --policy-name AmazonEKSLoadBalancerController --policy-document file://iam_policy.json
          else
            echo "‚úÖ IAM Policy already exists: $POLICY_ARN"
          fi
      
      - name: Ensure IAM Policy is Fully Available Before Attaching
        run: |
          POLICY_ARN=$(aws iam list-policies --query "Policies[?PolicyName=='AmazonEKSLoadBalancerController'].Arn" --output text)
          
          if [ -z "$POLICY_ARN" ]; then
            echo "‚ùå ERROR: IAM Policy not found. Exiting!"
            exit 1
          fi
      
          MAX_RETRIES=12  # Allow up to 10 minutes for AWS to propagate
          WAIT_TIME=30  # Start with 30 seconds, increase exponentially
      
          echo "‚úÖ Ensuring IAM policy exists before attaching..."
          for i in $(seq 1 $MAX_RETRIES); do
            if aws iam get-policy --policy-arn "$POLICY_ARN" > /dev/null 2>&1; then
              echo "‚úÖ IAM Policy found: $POLICY_ARN"
              echo "::set-output name=policy_arn::$POLICY_ARN"
              exit 0
            fi
            echo "‚è≥ IAM Policy not available yet. Retrying in $WAIT_TIME seconds..."
            sleep $WAIT_TIME
            WAIT_TIME=$((WAIT_TIME * 2))  # Exponential backoff: 30s, 60s, 120s, etc.
          done

          echo "‚ùå ERROR: IAM Policy did not become available in time!"
          exit 1



      # Step 11: Ensure IAM Policy is Fully Available Before Attaching with second step
      - name: Ensure IAM Policy is Fully Available Before Attachingsec
        run: |
          # POLICY_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${{ env.POLICY_NAME }}"
          POLICY_ARN=$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='$POLICY_NAME'].Arn" --output text)

          echo "POLICY_ARN=$POLICY_ARN" >> $GITHUB_ENV

          MAX_RETRIES=12  # Allow up to 10 minutes for AWS policy propagation
          WAIT_TIME=30  # Start with 30 seconds, increase exponentially

           if aws iam get-policy --policy-arn "$POLICY_ARN" > /dev/null 2>&1; then
                    echo "‚úÖ IAM Policy is now availablesec!"
                    echo "::set-output name=policy_arn::$POLICY_ARN"
                    exit 0
                  fi

          echo "‚úÖ Ensuring IAM policy exists before attaching..."
          for i in $(seq 1 $MAX_RETRIES); do
            if aws iam get-policy --policy-arn "$POLICY_ARN" > /dev/null 2>&1; then
              echo "‚úÖ IAM Policy is now available!"
              exit 0
            fi
            echo "‚è≥ IAM Policy not available yet. Retrying in $WAIT_TIME seconds..."
            sleep $WAIT_TIME
            WAIT_TIME=$((WAIT_TIME * 2))  # Exponential backoff (30s, 60s, 120s, etc.)
          done

          echo "‚ùå ERROR: IAM Policy did not become available in time!"
          exit 1     


      - name: Install AWS Load Balancer Controller
        run: |
          echo "Checking AWS Load Balancer Controller in cluster..."
          if ! kubectl get deployment -n kube-system aws-load-balancer-controller > /dev/null 2>&1; then
            echo "üöÄ Installing AWS Load Balancer Controller..."
            eksctl utils associate-iam-oidc-provider --region us-west-2 --cluster my-cluster --approve
            helm repo add eks https://aws.github.io/eks-charts
            helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
              --set clusterName=my-cluster \
              --set serviceAccount.create=true \
              --set region=us-west-2 \
              --set vpcId=$(aws ec2 describe-vpcs --query "Vpcs[0].VpcId" --output text --region us-west-2) \
              -n kube-system
          else
            echo "‚úÖ AWS Load Balancer Controller is already installed."
          fi
      
  
      - name: Login to AWS ECR
        run: |
          aws ecr get-login-password --region ${{ secrets.AWS_REGION }} | \
          docker login --username AWS --password-stdin ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_REGION }}.amazonaws.com
      
          kubectl create secret docker-registry ecr-secret \
            --docker-server="${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_REGION }}.amazonaws.com" \
            --docker-username=AWS \
            --docker-password="$(aws ecr get-login-password --region ${{ secrets.AWS_REGION }})" \
            --docker-email=none





      # 1Ô∏è‚É£1Ô∏è‚É£ Check ArgoCD Services and Pods
      - name: Check ArgoCD pods and services
        run: |
          kubectl get pods -n argocd
          kubectl get svc -n argocd

      # 1Ô∏è‚É£2Ô∏è‚É£ Ensure 'demo-app' Namespace Exists
      - name: Ensure demo-app Namespace Exists
        run: |
          if ! kubectl get namespace demo-app; then
            echo "Creating demo-app namespace..."
            kubectl create namespace demo-app
          else
            echo "Namespace demo-app already exists."
          fi

   
      # # 1Ô∏è‚É£5Ô∏è‚É£ Check all namespces 
      # - name: Check all namespces
      #   run: |
      #    kubectl get applications --all-namespaces
      #    kubectl get secret --all-namespaces | grep argocd-initial-admin-secret


      # 1Ô∏è‚É£6Ô∏è‚É£ Restart ArgoCD Pods if Needed
      - name: Restart ArgoCD Pods if Needed
        run: |
          kubectl -n argocd patch secret argocd-secret \
          -p '{"stringData": {
            "admin.password": "$2a$10$rRyBsGSHK6.uc8fntPwVIuLVHgsAhAX7TcdrqW/RADU0uh7CaChLa",
            "admin.passwordMtime": "'$(date +%FT%T%Z)'"
          }}'
          if [[ $(kubectl get pods -n argocd | grep -c "Running") -lt 5 ]]; then
            kubectl delete pod -n argocd --all
          fi

           # 5.1. Install ArgoCD in the 'argocd' namespace
      - name: Wait for ArgoCD Pods to be Ready
        run: |
          echo "Waiting for all ArgoCD pods to be ready..."
          while [[ $(kubectl get pods -n argocd --no-headers | grep -c -v "Running") -ne 0 ]]; do
            echo "Some pods are still not ready..."
            kubectl get pods -n argocd
            sleep 10
          done
          echo "All ArgoCD pods are running!"

      # 5.2. Install ArgoCD in the 'argocd' namespace
      - name: Wait for All ArgoCD Pods to be Ready
        run: |
          echo "Waiting for all ArgoCD pods to be ready..."
          for i in {1..30}; do
            READY_PODS=$(kubectl get pods -n argocd --no-headers | awk '{print $2}' | grep -c "1/1")
            TOTAL_PODS=$(kubectl get pods -n argocd --no-headers | wc -l)
      
            if [[ "$READY_PODS" -eq "$TOTAL_PODS" ]]; then
              echo "‚úÖ All ArgoCD pods are ready!"
              exit 0
            fi
      
            echo "‚è≥ Waiting... $READY_PODS/$TOTAL_PODS pods are ready."
            sleep 10
          done
    
          echo "‚ùå Error: ArgoCD pods failed to reach 1/1 READY state."





      - name: Wait for ArgoCD Readiness
        run: |
          kubectl rollout status deployment/argocd-server -n argocd --timeout=120s || \
          (echo "‚ùå ERROR: ArgoCD deployment is not ready" && exit 1)



      - name: Port Forward as Fallback
        if: failure()
        run: |
          echo "‚ö†Ô∏è Using port-forwarding as a fallback..."
          kubectl port-forward svc/argocd-server -n argocd 8080:443 & sleep 5
          argocd login localhost:8080 --username admin --password "password" --insecure
          # ARGOCD_ADMIN_PASSWORD=$(kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath="{.data.password}" | base64 --decode)
          echo "‚ö†Ô∏è Using port-forwarding as a fallback..."
          # echo ARGOCD_ADMIN_PASSWORD




      - name: Ensure AWS Load Balancer Webhook is Running
        run: |
          echo "üîÑ Checking AWS Load Balancer Webhook status..."
          for i in {1..5}; do
            if kubectl get svc aws-load-balancer-webhook-service -n kube-system > /dev/null 2>&1; then
              echo "‚úÖ AWS Load Balancer Webhook is available!"
              exit 0
            fi
            echo "‚è≥ Webhook service not available yet. Retrying in 20s..."
            sleep 20
          done
          echo "‚ùå ERROR: AWS Load Balancer Webhook is unavailable!"
          exit 1

      - name: Restart AWS Load Balancer Controller (if needed)
        run: |
          echo "üîÑ Restarting AWS Load Balancer Controller..."
          kubectl rollout restart deployment aws-load-balancer-controller -n kube-system
          sleep 30  # Give it time to restart
 
      




    # Step 3: Make NGINX Ingress Script Executable
      - name: Make NGINX Ingress Script Executable
        run: chmod +x scripts/03-install-nginx-ingress.sh

      # Step 4: Make ArgoCD ClusterIP Patch Script Executable
      - name: Make ArgoCD ClusterIP Patch Script Executable
        run: chmod +x scripts/04-patch-argocd-service.sh

      # Step 5: Make ArgoCD Ingress Script Executable
      - name: Make ArgoCD Ingress Script Executable
        run: chmod +x scripts/05-apply-argocd-ingress.sh

      # Step 6: Install NGINX Ingress Controller
      - name: Install NGINX Ingress Controller
        run: ./scripts/03-install-nginx-ingress.sh

      # Step 7: Patch ArgoCD Service to ClusterIP
      - name: Patch ArgoCD Service to ClusterIP
        run: ./scripts/04-patch-argocd-service.sh

      # - name: Get External IP or Hostname of NGINX Ingress Controller
      #   run: |
      #     echo "Getting External IP or Hostname for NGINX ingress controller..."
      #     for i in {1..20}; do
      #       EXTERNAL_IP=$(kubectl get svc ingress-nginx-controller -n ingress-nginx -o jsonpath="{.status.loadBalancer.ingress[0].ip}" 2>/dev/null || echo "")
      #       EXTERNAL_HOSTNAME=$(kubectl get svc ingress-nginx-controller -n ingress-nginx -o jsonpath="{.status.loadBalancer.ingress[0].hostname}" 2>/dev/null || echo "")
      #       if [[ -n "$EXTERNAL_IP" ]]; then
      #         echo "‚úÖ Ingress Controller External IP: $EXTERNAL_IP"
      #         echo "EXTERNAL_IP=$EXTERNAL_IP" >> $GITHUB_ENV
      #         exit 0
      #       elif [[ -n "$EXTERNAL_HOSTNAME" ]]; then
      #         echo "‚úÖ Ingress Controller External Hostname: $EXTERNAL_HOSTNAME"
      #         echo "EXTERNAL_HOSTNAME=$EXTERNAL_HOSTNAME" >> $GITHUB_ENV
      #         exit 0
      #       fi
      #       echo "‚è≥ Waiting for external IP/hostname... retry $i"
      #       sleep 15
      #     done
      #     echo "‚ùå Failed to get external IP/hostname for ingress controller"
      #     exit 1


      # # Step 9: Update Ingress YAML with External IP
      # - name: Update Ingress YAML with External IP
      #   run: |
      #     echo "Updating argocd-server-ingress.yaml with External IP: $EXTERNAL_IP"
      #     sed -i "s|<EXTERNAL_IP>|$EXTERNAL_IP|g" argocd/argocd-server-ingress.yaml

         # Step: Disable NGINX Admission Webhook (for testing only)
      - name: Disable NGINX Admission Webhook (testing only)
        run: |
          echo "‚ö†Ô∏è Deleting NGINX admission webhook temporarily for testing..."
          kubectl delete validatingwebhookconfiguration ingress-nginx-admission || echo "Webhook not found, continuing..."
      
      # Step: Apply ArgoCD Ingress
      - name: Apply ArgoCD Ingress
        run: ./scripts/05-apply-argocd-ingress.sh


    # Step 1: Wait for ArgoCD Server to Be Ready
      - name: Wait for ArgoCD Server to Be Ready
        run: |
          echo "üîÑ Waiting for ArgoCD server pod to be ready..."
          for i in {1..10}; do
            if kubectl get pods -n argocd | grep -q "argocd-server.*Running"; then
              echo "‚úÖ ArgoCD server is running!"
              exit 0
            fi
            echo "‚è≥ ArgoCD server not ready yet. Retrying in 10s..."
            sleep 10
          done
          echo "‚ùå ERROR: ArgoCD server did not start in time."
          exit 1
      
          # Step 2: Log into ArgoCD (with auto-detect fallback)
          - name: Log into ArgoCD (auto-detect server)
            run: |
              echo "üîç Detecting ArgoCD server address..."
          
              HOSTNAME=$(kubectl get svc ingress-nginx-controller -n ingress-nginx -o jsonpath="{.status.loadBalancer.ingress[0].hostname}" 2>/dev/null || echo "")
              IP=$(kubectl get svc ingress-nginx-controller -n ingress-nginx -o jsonpath="{.status.loadBalancer.ingress[0].ip}" 2>/dev/null || echo "")
              ARGOCD_SERVER="${HOSTNAME:-$IP}"
          
              if [[ -z "$ARGOCD_SERVER" ]]; then
                echo "‚ö†Ô∏è Could not detect external IP or hostname. Using port-forwarding fallback..."
                nohup kubectl port-forward svc/argocd-server -n argocd 8085:443 > /dev/null 2>&1 &
                sleep 5
                ARGOCD_SERVER="localhost:8085"
              fi
          
              echo "‚úÖ Logging into ArgoCD at: $ARGOCD_SERVER"
              echo "ARGOCD_SERVER=$ARGOCD_SERVER" >> $GITHUB_ENV
          
              for i in {1..5}; do
                if argocd login "$ARGOCD_SERVER" --username admin --password "password" --insecure; then
                  echo "‚úÖ Successfully logged into ArgoCD"
                  exit 0
                fi
                echo "‚è≥ Login failed. Retrying in 20s..."
                sleep 20
              done
          
              echo "‚ùå ERROR: ArgoCD login failed"
              exit 1
      


          
    
      - name: Start Port-Forwarding for ArgoCD
        run: |
          echo "üîÑ Starting port-forwarding to ArgoCD..."
          kubectl port-forward svc/argocd-server -n argocd 8085:443 > port_forward.log 2>&1 & echo $! > port_forward_pid
          sleep 5
          if ! ps -p $(cat port_forward_pid) > /dev/null; then
            echo "‚ùå Port-forwarding process failed to start"
            exit 1
          fi
          echo "‚úÖ Port-forwarding started successfully"


      - name: Ensure ArgoCD Server is Reachable
        run: |
          echo "üîç Checking if ArgoCD is reachable..."
          for i in {1..5}; do
            if nc -z localhost 8085; then
              echo "‚úÖ ArgoCD is accessible on localhost:8085"
              exit 0
            fi
            echo "‚è≥ Waiting for ArgoCD port-forward..."
            sleep 5
          done
          echo "‚ùå ERROR: Could not connect to ArgoCD on localhost:8085"

      - name: Log into ArgoCD via Port-Forward
        run: |
          echo "üîê Logging into ArgoCD via port-forward (localhost:8085)..."
          argocd login localhost:8085 --username admin --password "password" --insecure
      
      - name: Ensure ArgoCD CLI Recognizes 'admin'
        run: |
          echo "üîç Checking ArgoCD account permissions..."
          argocd account get --server=localhost:8085 --insecure || (echo "‚ùå 'admin' account missing or not reachable, failing job" && exit 1)
          echo "‚úÖ ArgoCD account verified"
                

      - name: Ensure 'default' Project Exists
        run: |
          echo "üîß Checking if 'default' project exists..."
          if ! argocd proj get default; then
            echo "üîÑ Creating 'default' project..."
            argocd proj create default
          fi
          echo "‚úÖ 'default' project exists"

      
      - name: üîê Full RBAC Reset and 'admin' Role Configuration
        run: |
          echo "üî• Removing legacy RBAC config (argocd-rbac-cm)..."
          kubectl delete configmap argocd-rbac-cm -n argocd || true
      
          echo "üí• Resetting 'admin' role in 'default' project..."
          argocd proj role delete default admin || true
          argocd proj role create default admin
      
          echo "‚úÖ Assigning valid wildcard application policies..."
          argocd proj role add-policy default admin -a get -o applications/* -p allow
          argocd proj role add-policy default admin -a sync -o applications/* -p allow
          argocd proj role add-policy default admin -a update -o applications/* -p allow
          argocd proj role add-policy default admin -a create -o applications/* -p allow
          argocd proj role add-policy default admin -a delete -o applications/* -p allow
      
          echo "‚úÖ Assigning explicit permissions for 'demo-app'..."
          argocd proj role add-policy default admin -a get -o applications/demo-app -p allow
          argocd proj role add-policy default admin -a update -o applications/demo-app -p allow
          argocd proj role add-policy default admin -a sync -o applications/demo-app -p allow
      
          echo "‚úÖ Assigning project-level permissions..."
          argocd proj role add-policy default admin -a get -o projects/default -p allow
          argocd proj role add-policy default admin -a update -o projects/default -p allow
      
          echo "‚úÖ Assigning cluster and namespace resource access..."
          argocd proj allow-cluster-resource default "*" "*"
          argocd proj allow-namespace-resource default "*" "*"
      
          echo "üîß Setting 'admin' as the default role for all users..."
          kubectl patch configmap argocd-cm -n argocd --type merge -p '{"data":{"policy.default":"role:admin"}}'
      
          echo "‚úÖ RBAC configuration and default role mapping completed successfully."
  
    
    
   
      


      - name: Wait for ArgoCD Server to Be Ready12
        run: |
          echo "üîÑ Waiting for ArgoCD server pods to be ready..."
          for i in {1..10}; do
            if kubectl get pods -n argocd | grep -q "argocd-server.*Running"; then
              echo "‚úÖ ArgoCD server is running!"
              exit 0
            fi
            echo "‚è≥ ArgoCD server not ready yet. Retrying in 10s..."
            sleep 10
          done
          echo "‚ùå ERROR: ArgoCD server did not start in time."
          exit 1

      - name: Start Port-Forwarding for ArgoCD
        run: |
          echo "üîÑ Checking if another process is using port 8085..."
          kill -9 $(lsof -t -i :8085) 2>/dev/null || true
          
          echo "üîÑ Checking available ports on argocd-server..."
          kubectl get svc argocd-server -n argocd -o yaml | grep "port:"

          echo "üîÑ Starting port-forwarding to ArgoCD..."
          nohup kubectl port-forward svc/argocd-server -n argocd 8085:443 > port_forward.log 2>&1 & echo $! > port_forward_pid
          sleep 5
          
          if ! ps -p $(cat port_forward_pid) > /dev/null; then
            echo "‚ùå Port-forwarding process failed to start"
            exit 1
          fi
          
          echo "‚úÖ Port-forwarding started successfully"

      - name: Ensure ArgoCD Server is Reachable
        run: |
          echo "üîç Checking if ArgoCD is reachable..."
          for i in {1..5}; do
            if nc -z localhost 8085; then
              echo "‚úÖ ArgoCD is accessible on localhost:8085"
              exit 0
            fi
            echo "‚è≥ Waiting for ArgoCD port-forward..."
            sleep 5
          done
          echo "‚ùå ERROR: Could not connect to ArgoCD on localhost:8085"
          exit 1
      - name: Ensure ArgoCD Server is Fully Ready
        run: |
          echo "üîç Waiting for ArgoCD API to become responsive..."
          for i in {1..10}; do
            if curl -k https://localhost:8085/healthz; then
              echo "‚úÖ ArgoCD is ready!"
              exit 0
            fi
            echo "‚è≥ Waiting for ArgoCD readiness... retrying in 5s"
            sleep 5
          done
          echo "‚ùå ArgoCD did not respond to health check in time."
          exit 1
      
      - name: Log into ArgoCD (Ignoring Certificate Errors1)
        run: |
          echo "üîë Attempting to log into ArgoCD..."
          for i in {1..5}; do
            if argocd login localhost:8085 \
              --username admin \
              --password "password" \
              --insecure; then
              echo "‚úÖ Successfully logged into ArgoCD"
              exit 0
            fi
            echo "‚è≥ Login failed. Retrying in 20s..."
            sleep 20
          done
          echo "‚ùå ERROR: ArgoCD login failed"
          exit 1

      - name: üß™ Verify Assigned Roles in Current Token
        run: |
          echo "üîç Verifying token roles..."
          argocd account get --server=localhost:8085

      - name: üß™ Verify Assigned Roles in Current Token
        run: |
          echo "üîç Verifying token roles..."
          argocd account get --server=localhost:8085


      # - name: Assign Full Sync Permissions to 'admin' Role
      #   run: |
      #     echo "üîß Assigning full sync permissions to 'admin' role..."
      #     argocd proj role add-policy default admin -a get -o applications/* -p allow
      #     argocd proj role add-policy default admin -a sync -o applications/* -p allow
      #     argocd proj role add-policy default admin -a update -o applications/* -p allow
      #     argocd proj role add-policy default admin -a override -o applications/* -p allow
      #     argocd proj role add-policy default admin -a create -o applications/* -p allow
      #     argocd proj role add-policy default admin -a delete -o applications/* -p allow
      #     echo "‚úÖ Permissions updated"


      # - name: Assign Full Project-Level Permissions to 'admin' Role
      #   run: |
      #     echo "üîß Assigning project-level permissions to 'admin' role..."
      #     argocd proj role add-policy default admin -a get -o projects/default -p allow
      #     argocd proj role add-policy default admin -a update -o projects/default -p allow
      #     argocd proj role add-policy default admin -a sync -o projects/default -p allow
      #     argocd proj role add-policy default admin -a override -o projects/default -p allow
      #     echo "‚úÖ Project-level permissions assigned"

      # - name: Assign Application-Level Access to 'admin' Role
      #   run: |
      #     echo "üîß Assigning explicit permissions for 'admin' role on 'demo-app'..."
      #     argocd proj role add-policy default admin -a get -o applications/demo-app -p allow
      #     argocd proj role add-policy default admin -a update -o applications/demo-app -p allow
      #     argocd proj role add-policy default admin -a sync -o applications/demo-app -p allow
  
       

      #     echo "‚úÖ 'admin' role permissions for 'demo-app' updated"


      # - name: Assign Namespace & Cluster-Level Access
      #   run: |
      #     echo "üîß Assigning namespace and cluster-level access to 'admin' role..."
      #     argocd proj allow-cluster-resource default "*" "*"
      #     argocd proj allow-namespace-resource default "*" "*"
      #     echo "‚úÖ Namespace and cluster-wide access granted"



      
      - name: Ensure Global ArgoCD Authorization for 'admin'
        run: |
          echo "üîß Ensuring 'admin' is globally authorized in ArgoCD..."
          kubectl patch configmap argocd-cm -n argocd --type merge -p \
            '{"data":{"policy.default":"role:admin"}}'
          echo "‚úÖ Global authorization applied"
          
      # - name: üîß Update ArgoCD RBAC settings
      #   run: |
      #     echo "üîß Updating ArgoCD RBAC settings..."
      
      #     kubectl patch configmap argocd-rbac-cm -n argocd --type merge -p \
      #     '{"data": {"policy.csv": "g, my-group, role:admin\ng, dev-team, role:readonly"}}'
      
      #     echo "‚úÖ ArgoCD RBAC settings updated successfully!"


          
      - name: Ensure ArgoCD Controller Has Full Cluster Access
        run: |
          echo "üîß Granting cluster-admin access to ArgoCD controller..."
          kubectl create clusterrolebinding argocd-admin-binding --clusterrole=cluster-admin --serviceaccount=argocd:argocd-application-controller || true
          echo "‚úÖ ArgoCD controller has cluster-admin access"

          


      
      # - name: Restart ArgoCD Server to Apply Authorization Changes
      #   run: |
      #     echo "üîÑ Restarting ArgoCD to apply changes..."
      #     kubectl rollout restart deployment argocd-server -n argocd
      #     sleep 30
      #     echo "‚úÖ Restart complete"





      - name: Verify Port-Forwarding is Still Active
        run: |
          echo "üîç Checking if port-forwarding is still active..."
          if ! ps -p $(cat port_forward_pid) > /dev/null; then
            echo "‚ö†Ô∏è Port-forwarding process is missing. Restarting..."
            kubectl port-forward svc/argocd-server -n argocd 8085:443 > port_forward.log 2>&1 & echo $! > port_forward_pid
            sleep 5
          fi
          echo "‚úÖ Port-forwarding process is running"

      - name: Debug - Verify 'admin' Role Permissions in ArgoCD
        run: |
          echo "üîç Checking 'admin' role permissions in ArgoCD..."
          argocd proj role get default admin --server=localhost:8085 || (echo "‚ùå 'admin' role missing or has incorrect permissions!" && exit 1)
          echo "‚úÖ 'admin' role permissions verified"

      - name: üîç Checking if 'demo-app' exists...
        run: argocd app list

              
      - name: Recreate ArgoCD Application
        run: |
          echo "üöÄ Recreating ArgoCD application..."
          kubectl delete -f argocd/application.yaml --ignore-not-found
          sed -i "s|AWS_ACCOUNT_ID|${{ secrets.AWS_ACCOUNT_ID }}|g" argocd/application.yaml
          kubectl apply -f argocd/application.yaml

          kubectl apply -f argocd/application.yaml

      - name: üîç Checking if 'demo-app' exists222..
        run: argocd app list    

          
      - name: Debug - Check ArgoCD Logs for RBAC Issues
        run: |
          echo "üîç Checking ArgoCD logs for RBAC rejections..."
          kubectl logs -n argocd -l app.kubernetes.io/name=argocd-server | grep "RBAC" || echo "‚úÖ No RBAC rejections found"
    
      - name: Debug - List Applications to Check Ownership
        run: |
          echo "üîç Listing ArgoCD applications..."
          argocd app list --server=localhost:8085
          echo "‚úÖ ArgoCD applications listed"
          
      - name: Debug - Verify 'demo-app' Exists Before Assignment
        run: |
          echo "üîç Checking if 'demo-app' exists..."
          if ! argocd app get demo-app --server=localhost:8085; then
            echo "‚ö†Ô∏è 'demo-app' does not exist or is inaccessible (likely RBAC issue)"
            exit 0  # Soft-exit instead of breaking the build
          fi
          echo "‚úÖ 'demo-app' exists and is accessible."


      - name: Debug - Check If 'admin' Can Access 'demo-app'
        run: |
          echo "üîç Checking if 'admin' can get 'demo-app'..."
          if ! argocd app get demo-app --server=localhost:8085; then
            echo "‚ùå 'admin' cannot access 'demo-app'. This is likely an RBAC issue!"
            argocd account get --server=localhost:8085
            exit 0  # Don't fail the pipeline, just log it
          fi
          echo "‚úÖ 'admin' has access to 'demo-app'"
                
      - name: Ensure 'demo-app' Is Assigned to 'default' Project
        run: |
          echo "üîç Checking if 'admin' owns 'demo-app'..."
          argocd app set demo-app --project default --server=localhost:8085
          echo "‚úÖ 'demo-app' ownership reassigned to 'default' proj



      - name: Ensure 'demo-app' Ownership
        run: |
          echo "üîç Checking if 'admin' owns 'demo-app'..."
          argocd app get demo-app --server=localhost:8085 || \
          argocd app set demo-app --project default --server=localhost:8085
          echo "‚úÖ Ownership verified"

    
      - name: Verify 'admin' Role Permissions
        run: |
          echo "üîç Verifying 'admin' role permissions..."
          argocd proj role get default admin

      - name: Sync ArgoCD Application
        run: |
          echo "üöÄ Syncing demo-app with ArgoCD..."
          argocd app sync demo-app --server=$ARGOCD_SERVER
          echo "‚úÖ Sync initiated successfully"

      - name: Port Forward as Fallback
        if: failure()
        run: |
          echo "‚ö†Ô∏è Using port-forwarding as a fallback..."
          kubectl port-forward svc/argocd-server -n argocd 8085:443 > /dev/null 2>&1 & echo $! > port_forward_pid
          sleep 5

          echo "üîë Logging into ArgoCD via port-forward..."
          argocd login localhost:8085 --username admin --password "password" --insecure
          echo "‚úÖ Successfully logged in using port-forwarding"

          echo "üöÄ Syncing demo-app with ArgoCD via port-forward..."
          argocd app sync demo-app --server=localhost:8085 || (echo "‚ùå Sync failed via port-forwarding." && exit 1)
          echo "‚úÖ Sync initiated successfully (fallback mode)Configure RBAC Permissions for 'admin' Role in ArgoCD"

          echo "üõë Cleaning up port-forwarding..."
          kill $(cat port_forward_pid)


    
