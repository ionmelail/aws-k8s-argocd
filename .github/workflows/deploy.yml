name: Deploy EKS & ArgoCD

on:
  push:
    branches:
      - nodejs_nginx

jobs:
  deploy:
    runs-on: ubuntu-latest
    env:
      AWS_REGION: us-west-2
      CLUSTER_NAME: my-cluster
      POLICY_NAME: AmazonEKSLoadBalancerController
      SERVICE_ACCOUNT_NAMESPACE: kube-system
      SERVICE_ACCOUNT_NAME: aws-load-balancer-controller
    steps:
    
      # 1Ô∏è‚É£ Checkout Repository Code12112
      - name: Checkout Code
        uses: actions/checkout@v3
 
      #  Step 2: Configure AWS Credentials using GitHub Secrets
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2
  

      #  Step 3: Set up Terraform for Infrastructure Provisioning
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v2

      # 4Ô∏è‚É£ Make Deployment Script Executable & Run It
      - name: Make Deployment Script Executable
        run: chmod +x scripts/Aws_K8s_Argocd.sh  

      - name: Run Deployment Script
        run: ./scripts/Aws_K8s_Argocd.sh  

      # 5Ô∏è‚É£ Initialize and Apply Terraform Configuration for EKS
      - name: Terraform Init & Apply
        run: |
          cd terraform
          terraform init
          terraform apply -auto-approve

      # 6Ô∏è‚É£ Update Kubeconfig to Access EKS Cluster
      - name: Update kubeconfig
        run: aws eks --region us-west-2 update-kubeconfig --name my-cluster

      # 7Ô∏è‚É£ Create ArgoCD Namespace if Not Exists
      - name: Create ArgoCD namespace
        run: |
          kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -

      # 8Ô∏è‚É£ Ensure ArgoCD Admin Secret Exists
      - name: Ensure ArgoCD Secret Exists
        run: |
          if ! kubectl get secret argocd-secret -n argocd; then
            kubectl create secret generic argocd-secret -n argocd --from-literal=admin.password='$2a$10$wEJ.NXBfjRj9JQ0QeqA1OuD4/2H6pRxH3p80fD/QFOhH8sD/jq12y'
          fi



      # 9Ô∏è‚É£ Install ArgoCD in the 'argocd' Namespace
      - name: Install ArgoCD
        run: |
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

       # 10. Wait ArgoCD in the 'argocd' namespace
      - name: Wait for ArgoCD Pods to be Ready
        run: |
          echo "Waiting for all ArgoCD pods to be ready..."
          while [[ $(kubectl get pods -n argocd --no-headers | grep -c -v "Running") -ne 0 ]]; do
            echo "Some pods are still not ready..."
            kubectl get pods -n argocd
            kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

            sleep 10
          done
          echo "All ArgoCD pods are running!"

      # 11. Wait ArgoCD in the 'argocd' namespace
      - name: Wait for All ArgoCD Pods to be Ready
        run: |
          echo "Waiting for all ArgoCD pods to be ready..."
          for i in {1..30}; do
            READY_PODS=$(kubectl get pods -n argocd --no-headers | awk '{print $2}' | grep -c "1/1")
            TOTAL_PODS=$(kubectl get pods -n argocd --no-headers | wc -l)
      
            if [[ "$READY_PODS" -eq "$TOTAL_PODS" ]]; then
              echo "‚úÖ All ArgoCD pods are ready!"
              exit 0
            fi
      
            echo "‚è≥ Waiting... $READY_PODS/$TOTAL_PODS pods are ready."
            sleep 10
          done
    
          echo "‚ùå Error: ArgoCD pods failed to reach 1/1 READY state."

      # # ‚úÖ Install eksctl (Required for Load Balancer Controller)
      # - name: Install eksctl
      #   run: |
      #     echo "Installing eksctl..."
      #     curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
      #     sudo mv /tmp/eksctl /usr/local/bin
      #     eksctl version
      #     echo "‚úÖ eksctl installed successfully!"




      # üîπ Step 1: Ensure AWS Load Balancer Controller IAM Policy Exists
      - name: Ensure AWS Load Balancer Controller IAM Policy Exists
        run: |
          echo "üîç Checking AWS IAM Policy for LoadBalancer Controller..."
          POLICY_ARN=$(aws iam list-policies --query "Policies[?PolicyName=='AmazonEKSLoadBalancerController'].Arn" --output text)
          if [ -z "$POLICY_ARN" ]; then
            echo "‚ö†Ô∏è IAM Policy not found! Creating policy..."
            curl -s -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json
            aws iam create-policy --policy-name AmazonEKSLoadBalancerController --policy-document file://iam_policy.json
          else
            echo "‚úÖ IAM Policy already exists: $POLICY_ARN"
          fi
      
      - name: Ensure IAM Policy is Fully Available Before Attaching
        run: |
          POLICY_ARN=$(aws iam list-policies --query "Policies[?PolicyName=='AmazonEKSLoadBalancerController'].Arn" --output text)
          
          if [ -z "$POLICY_ARN" ]; then
            echo "‚ùå ERROR: IAM Policy not found. Exiting!"
            exit 1
          fi
      
          MAX_RETRIES=12  # Allow up to 10 minutes for AWS to propagate
          WAIT_TIME=30  # Start with 30 seconds, increase exponentially
      
          echo "‚úÖ Ensuring IAM policy exists before attaching..."
          for i in $(seq 1 $MAX_RETRIES); do
            if aws iam get-policy --policy-arn "$POLICY_ARN" > /dev/null 2>&1; then
              echo "‚úÖ IAM Policy found: $POLICY_ARN"
              echo "::set-output name=policy_arn::$POLICY_ARN"
              exit 0
            fi
            echo "‚è≥ IAM Policy not available yet. Retrying in $WAIT_TIME seconds..."
            sleep $WAIT_TIME
            WAIT_TIME=$((WAIT_TIME * 2))  # Exponential backoff: 30s, 60s, 120s, etc.
          done

          echo "‚ùå ERROR: IAM Policy did not become available in time!"
          exit 1



      # Step 11: Ensure IAM Policy is Fully Available Before Attaching with second step
      - name: Ensure IAM Policy is Fully Available Before Attachingsec
        run: |
          # POLICY_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${{ env.POLICY_NAME }}"
          POLICY_ARN=$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='$POLICY_NAME'].Arn" --output text)

          echo "POLICY_ARN=$POLICY_ARN" >> $GITHUB_ENV

          MAX_RETRIES=12  # Allow up to 10 minutes for AWS policy propagation
          WAIT_TIME=30  # Start with 30 seconds, increase exponentially

           if aws iam get-policy --policy-arn "$POLICY_ARN" > /dev/null 2>&1; then
                    echo "‚úÖ IAM Policy is now availablesec!"
                    echo "::set-output name=policy_arn::$POLICY_ARN"
                    exit 0
                  fi

          echo "‚úÖ Ensuring IAM policy exists before attaching..."
          for i in $(seq 1 $MAX_RETRIES); do
            if aws iam get-policy --policy-arn "$POLICY_ARN" > /dev/null 2>&1; then
              echo "‚úÖ IAM Policy is now available!"
              exit 0
            fi
            echo "‚è≥ IAM Policy not available yet. Retrying in $WAIT_TIME seconds..."
            sleep $WAIT_TIME
            WAIT_TIME=$((WAIT_TIME * 2))  # Exponential backoff (30s, 60s, 120s, etc.)
          done

          echo "‚ùå ERROR: IAM Policy did not become available in time!"
          exit 1     


      # - name: Install AWS Load Balancer Controller
      #   run: |
      #     echo "Checking AWS Load Balancer Controller in cluster..."
      #     if ! kubectl get deployment -n kube-system aws-load-balancer-controller > /dev/null 2>&1; then
      #       echo "üöÄ Installing AWS Load Balancer Controller..."
      #       eksctl utils associate-iam-oidc-provider --region us-west-2 --cluster my-cluster --approve
      #       helm repo add eks https://aws.github.io/eks-charts
      #       helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
      #         --set clusterName=my-cluster \
      #         --set serviceAccount.create=true \
      #         --set region=us-west-2 \
      #         --set vpcId=$(aws ec2 describe-vpcs --query "Vpcs[0].VpcId" --output text --region us-west-2) \
      #         -n kube-system
      #     else
      #       echo "‚úÖ AWS Load Balancer Controller is already installed."
      #     fi

      - name: Make AWS LB Controller Script Executable
        run: chmod +x scripts/02-install-aws-lb-controller.sh
      
      - name: Install AWS Load Balancer Controller
        run: ./scripts/02-install-aws-lb-controller.sh
        
      - name: Make NGINX Ingress Script Executable
        run: chmod +x scripts/03-install-nginx-ingress.sh
      
      - name: Make ArgoCD ClusterIP Patch Script Executable
        run: chmod +x scripts/04-patch-argocd-service.sh
      
      - name: Make ArgoCD Ingress Script Executable
        run: chmod +x scripts/05-apply-argocd-ingress.sh
      
      # Now safe to run all of them:
      - name: Install NGINX Ingress Controller
        run: ./scripts/03-install-nginx-ingress.sh
      
      - name: Patch ArgoCD Service to ClusterIP
        run: ./scripts/04-patch-argocd-service.sh
      
      - name: Apply ArgoCD Ingress
        run: ./scripts/05-apply-argocd-ingress.sh
            
  # # Step 12: Attach IAM Policy to EKS Node Role
  #     - name: Attach IAM Policy to EKS Node Role
  #       run: |
  #         echo "üîç Attaching IAM policy to EKS Node Role..."
  #         NODE_ROLE=$(aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --query "cluster.roleArn" --output text | awk -F '/' '{print $2}')
  #         POLICY_ARN=$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='$POLICY_NAME'].Arn" --output text)

  #         # Ensure policy exists before attaching
  #         if ! aws iam get-policy --policy-arn "$POLICY_ARN" > /dev/null 2>&1; then
  #           echo "‚ùå ERROR: IAM Policy does not exist!"
  #           exit 1
  #         fi

  #         # Check if policy is already attached
  #         if aws iam list-attached-role-policies --role-name "$NODE_ROLE" --query "AttachedPolicies[?PolicyArn=='$POLICY_ARN']" --output text | grep -q "$POLICY_ARN"; then
  #           echo "‚úÖ IAM Policy is already attached to $NODE_ROLE."
  #         else
  #           echo "üîÑ Attaching IAM Policy to $NODE_ROLE..."
  #           aws iam attach-role-policy --policy-arn "$POLICY_ARN" --role-name "$NODE_ROLE"
  #           echo "‚úÖ IAM Policy successfully attached to role: $NODE_ROLE"
  #         fi
      
  #     # üîπ Step 4: Install AWS Load Balancer Controller (AFTER Policy is Attached)
  #     - name: Install AWS Load Balancer Controller
  #       run: |
  #         echo "Checking AWS Load Balancer Controller in cluster..."
  #         if ! kubectl get deployment -n kube-system aws-load-balancer-controller > /dev/null 2>&1; then
  #           echo "üöÄ Installing AWS Load Balancer Controller..."
  #           eksctl utils associate-iam-oidc-provider --region us-west-2 --cluster my-cluster --approve
  #           helm repo add eks https://aws.github.io/eks-charts
  #           helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  #             --set clusterName=my-cluster \
  #             --set serviceAccount.create=true \
  #             --set region=us-west-2 \
  #             --set vpcId=$(aws ec2 describe-vpcs --query "Vpcs[0].VpcId" --output text --region us-west-2) \
  #             -n kube-system
  #         else
  #           echo "‚úÖ AWS Load Balancer Controller is already installed."
  #         fi

  
      - name: Login to AWS ECR
        run: |
          aws ecr get-login-password --region ${{ secrets.AWS_REGION }} | \
          docker login --username AWS --password-stdin ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_REGION }}.amazonaws.com
      
          kubectl create secret docker-registry ecr-secret \
            --docker-server="${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_REGION }}.amazonaws.com" \
            --docker-username=AWS \
            --docker-password="$(aws ecr get-login-password --region ${{ secrets.AWS_REGION }})" \
            --docker-email=none


      # ‚úÖ Step 11: Apply LoadBalancer for ArgoCD Server
      - name: Apply LoadBalancer to ArgoCD Server
        run: |
          echo "Applying LoadBalancer to ArgoCD server..."
          kubectl apply -f argocd/argocd-server-loadbalancer.yaml
          echo "‚úÖ LoadBalancer configuration applied!"

      # ‚úÖ Step 12: Ensure argocd-redis Secret Exists BEFORE ArgoCD Restarts
      - name: Ensure ArgoCD Redis Secret Exists
        run: |
          if ! kubectl get secret argocd-redis -n argocd; then
            kubectl create secret generic argocd-redis -n argocd --from-literal=password=$(openssl rand -base64 32)
          fi



      # # 1Ô∏è‚É£1Ô∏è‚É£ Check ArgoCD Services and Pods
      # - name: Check ArgoCD pods and services
      #   run: |
      #     kubectl get pods -n argocd
      #     kubectl get svc -n argocd

      # 1Ô∏è‚É£2Ô∏è‚É£ Ensure 'demo-app' Namespace Exists
      - name: Ensure demo-app Namespace Exists
        run: |
          if ! kubectl get namespace demo-app; then
            echo "Creating demo-app namespace..."
            kubectl create namespace demo-app
          else
            echo "Namespace demo-app already exists."
          fi

   
      # # 1Ô∏è‚É£5Ô∏è‚É£ Check all namespces 
      # - name: Check all namespces
      #   run: |
      #    kubectl get applications --all-namespaces
      #    kubectl get secret --all-namespaces | grep argocd-initial-admin-secret


      # 1Ô∏è‚É£6Ô∏è‚É£ Restart ArgoCD Pods if Needed
      - name: Restart ArgoCD Pods if Needed
        run: |
          kubectl -n argocd patch secret argocd-secret \
          -p '{"stringData": {
            "admin.password": "$2a$10$rRyBsGSHK6.uc8fntPwVIuLVHgsAhAX7TcdrqW/RADU0uh7CaChLa",
            "admin.passwordMtime": "'$(date +%FT%T%Z)'"
          }}'
          if [[ $(kubectl get pods -n argocd | grep -c "Running") -lt 5 ]]; then
            kubectl delete pod -n argocd --all
          fi

           # 5.1. Install ArgoCD in the 'argocd' namespace
      - name: Wait for ArgoCD Pods to be Ready
        run: |
          echo "Waiting for all ArgoCD pods to be ready..."
          while [[ $(kubectl get pods -n argocd --no-headers | grep -c -v "Running") -ne 0 ]]; do
            echo "Some pods are still not ready..."
            kubectl get pods -n argocd
            sleep 10
          done
          echo "All ArgoCD pods are running!"

      # 5.2. Install ArgoCD in the 'argocd' namespace
      - name: Wait for All ArgoCD Pods to be Ready
        run: |
          echo "Waiting for all ArgoCD pods to be ready..."
          for i in {1..30}; do
            READY_PODS=$(kubectl get pods -n argocd --no-headers | awk '{print $2}' | grep -c "1/1")
            TOTAL_PODS=$(kubectl get pods -n argocd --no-headers | wc -l)
      
            if [[ "$READY_PODS" -eq "$TOTAL_PODS" ]]; then
              echo "‚úÖ All ArgoCD pods are ready!"
              exit 0
            fi
      
            echo "‚è≥ Waiting... $READY_PODS/$TOTAL_PODS pods are ready."
            sleep 10
          done
    
          echo "‚ùå Error: ArgoCD pods failed to reach 1/1 READY state."



      # # ‚úÖ Step 16: Wait for ArgoCD Server LoadBalancer IP (Retries)
      # - name: Wait for ArgoCD Server LoadBalancer
      #   run: |
      #     echo "Waiting for ArgoCD server LoadBalancer IP..."
      #     for i in {1..30}; do
      #       ARGOCD_SERVER=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
      #       if [[ -n "$ARGOCD_SERVER" ]]; then
      #         echo "‚úÖ ArgoCD Server Ready: $ARGOCD_SERVER"
      #         echo "ARGOCD_SERVER=$ARGOCD_SERVER" >> $GITHUB_ENV
      #         exit 0
      #       fi
      #       echo "‚è≥ ArgoCD server address not available yet. Retrying in 10s..."
      #       sleep 10
      #     done
      #     echo "‚ùå ERROR: ArgoCD server LoadBalancer IP not found. Check Kubernetes events."

      - name: Wait for NGINX Ingress External IP
        run: |
          echo "Waiting for NGINX ingress external IP..."
          for i in {1..30}; do
            INGRESS_IP=$(kubectl get svc ingress-nginx-controller -n ingress-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || true)
            if [[ -n "$INGRESS_IP" ]]; then
              echo "‚úÖ NGINX Ingress is ready: $INGRESS_IP"
              echo "INGRESS_IP=$INGRESS_IP" >> $GITHUB_ENV
              exit 0
            fi
            echo "‚è≥ Ingress IP not available yet. Retrying in 10s..."
            sleep 10
          done
          echo "‚ùå ERROR: NGINX Ingress external IP not available."
          exit 1


          
      #     kubectl get events -n argocd --sort-by=.metadata.creationTimestamp
      #     exit 1

      # - name: Check DNS Resolution
      #   run: |
      #     nslookup $ARGOCD_SERVER || (echo "‚ùå ERROR: DNS resolution failed." && exit 1)

      - name: Wait for ArgoCD Readiness
        run: |
          kubectl rollout status deployment/argocd-server -n argocd --timeout=120s || \
          (echo "‚ùå ERROR: ArgoCD deployment is not ready" && exit 1)

      # - name: Log into ArgoCD
      #   run: |
      #     argocd login $ARGOCD_SERVER --username admin --password "password" --insecure
      #     echo "‚úÖ Successfully logged into ArgoCD"

      # - name: Port Forward as Fallback
      #   if: failure()
      #   run: |
      #     echo "‚ö†Ô∏è Using port-forwarding as a fallback..."
      #     kubectl port-forward svc/argocd-server -n argocd 8080:443 & sleep 5
      #     argocd login localhost:8080 --username admin --password "password" --insecure
      #     # ARGOCD_ADMIN_PASSWORD=$(kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath="{.data.password}" | base64 --decode)
      #     echo "‚ö†Ô∏è Using port-forwarding as a fallback..."
      #     # echo ARGOCD_ADMIN_PASSWORD

          
      # # ‚úÖ de aicic in jos Step 14: Get ArgoCD Server Address (with Wait Loop)
      # - name: Get ArgoCD Server Address
      #   run: |
      #     echo "Waiting for ArgoCD server address..."
      #     for i in {1..30}; do
      #       ARGOCD_SERVER=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
      #       if [[ -n "$ARGOCD_SERVER" ]]; then
      #         echo "‚úÖ ArgoCD Server Address: $ARGOCD_SERVER"
      #         echo "ARGOCD_SERVER=$ARGOCD_SERVER" >> $GITHUB_ENV
      #         exit 0
      #       fi
      #       echo "‚è≥ ArgoCD server address not available yet. Retrying in 10s..."
      #       sleep 10
      #     done
      #     echo "‚ùå ERROR: ArgoCD server address not found after waiting."
      #     exit 1


      # # 1Ô∏è‚É£8Ô∏è‚É£ Retrieve ArgoCD Admin Password
      # - name: Get ArgoCD Admin Password
      #   run: |
      #     echo "update ArgoCD admin password..."
      #     kubectl -n argocd patch secret argocd-secret \
      #     -p '{"stringData": {
      #       "admin.password": "$2a$10$rRyBsGSHK6.uc8fntPwVIuLVHgsAhAX7TcdrqW/RADU0uh7CaChLa",
      #       "admin.passwordMtime": "'$(date +%FT%T%Z)'"
      #     }}'
      #     echo "Retrieving ArgoCD admin password..."
                                 

      #     # ARGOCD_ADMIN_PASSWORD=$(kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath="{.data.password}" | base64 --decode)
      #     ARGOCD_ADMIN_PASSWORD='password'
      #     if [[ -z "$ARGOCD_ADMIN_PASSWORD" ]]; then
      #       echo "‚ùå ERROR: Unable to retrieve ArgoCD admin password."
      #       exit 1
      #     fi
      #     echo "::add-mask::$ARGOCD_ADMIN_PASSWORD"
      #     echo "ARGOCD_ADMIN_PASSWORD=$ARGOCD_ADMIN_PASSWORD" >> $GITHUB_ENV

      # # 1Ô∏è‚É£9Ô∏è‚É£ Log in to ArgoCD
      # - name: Login to ArgoCD
      #   run: |
      #     echo "Logging into ArgoCD..."
      #     argocd login "$ARGOCD_SERVER" --username admin --password "$ARGOCD_ADMIN_PASSWORD" --insecure
      #     echo "‚úÖ Successfully logged into ArgoCD."

      - name: Ensure AWS Load Balancer Webhook is Running
        run: |
          echo "üîÑ Checking AWS Load Balancer Webhook status..."
          for i in {1..5}; do
            if kubectl get svc aws-load-balancer-webhook-service -n kube-system > /dev/null 2>&1; then
              echo "‚úÖ AWS Load Balancer Webhook is available!"
              exit 0
            fi
            echo "‚è≥ Webhook service not available yet. Retrying in 20s..."
            sleep 20
          done
          echo "‚ùå ERROR: AWS Load Balancer Webhook is unavailable!"
          exit 1

      - name: Restart AWS Load Balancer Controller (if needed)
        run: |
          echo "üîÑ Restarting AWS Load Balancer Controller..."
          kubectl rollout restart deployment aws-load-balancer-controller -n kube-system
          sleep 30  # Give it time to restart
 
      
      # - name: Apply ArgoCD Application 
      #   run: |
      #     echo "üöÄ Applying ArgoCD application..."
      #     sed -i "s|AWS_ACCOUNT_ID|${{ secrets.AWS_ACCOUNT_ID }}|g" argocd/application.yaml

      #     kubectl apply -f argocd/application.yaml

      - name: Make ArgoCD Application Script Executable
        run: chmod +x scripts/07-apply-argocd-app.sh
      
      - name: Apply ArgoCD Application
        run: ./scripts/07-apply-argocd-app.sh ${{ secrets.AWS_ACCOUNT_ID }}

      - name: Make ArgoCD Ingress Patch Script Executable
        run: chmod +x scripts/08-patch-argocd-ingress.sh
      
      - name: Patch ArgoCD Ingress with Real Host
        run: ./scripts/08-patch-argocd-ingress.sh


      # - name: Get ArgoCD Server Hostname
      #   run: |
      #     echo "üîÑ Waiting for ArgoCD LoadBalancer hostname..."
      #     for i in {1..10}; do
      #       ARGOCD_SERVER=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
      #       if [[ -n "$ARGOCD_SERVER" ]]; then
      #         echo "‚úÖ ArgoCD LoadBalancer: $ARGOCD_SERVER"
      #         echo "ARGOCD_SERVER=$ARGOCD_SERVER" >> $GITHUB_ENV
      #         exit 0
      #       fi
      #       echo "‚è≥ LoadBalancer hostname not available yet. Retrying in 30s..."
      #       sleep 30
      #     done
      #     echo "‚ùå ERROR: ArgoCD LoadBalancer hostname not assigned."
      #     exit 1

      - name: Get NGINX Ingress External IP for ArgoCD
        run: |
          echo "üîç Fetching Ingress IP for ArgoCD..."
          for i in {1..10}; do
            INGRESS_IP=$(kubectl get svc ingress-nginx-controller -n ingress-nginx -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || true)
            if [[ -n "$INGRESS_IP" ]]; then
              echo "‚úÖ Found NGINX Ingress IP: $INGRESS_IP"
              echo "ARGOCD_NIP_URL=http://argocd.${INGRESS_IP}.nip.io" >> $GITHUB_ENV
              exit 0
            fi
            echo "‚è≥ Ingress IP not available. Retrying in 10s..."
            sleep 10
          done
      
          echo "‚ùå ERROR: Ingress IP not assigned."
          exit 1

      # - name: Wait for ArgoCD LoadBalancer Provisioning
      #   run: |
      #     echo "üîÑ Checking if ArgoCD LoadBalancer is ready..."
      #     for i in {1..10}; do
      #       ARGOCD_SERVER=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
      #       if [[ -n "$ARGOCD_SERVER" ]]; then
      #         echo "‚úÖ ArgoCD LoadBalancer: $ARGOCD_SERVER"
      #         echo "ARGOCD_SERVER=$ARGOCD_SERVER" >> $GITHUB_ENV
      #         exit 0
      #       fi
      #       echo "‚è≥ LoadBalancer not ready yet. Retrying in 30s..."
      #       sleep 30
      #     done
      #     echo "‚ùå ERROR: ArgoCD LoadBalancer hostname not assigned."
      #     exit 1



      # - name: Wait for ArgoCD LoadBalancer to be Reachable
      #   run: |
      #     echo "üîÑ Checking if ArgoCD LoadBalancer is ready..."
      #     for i in {1..10}; do
      #       ARGOCD_SERVER=$(kubectl get svc argocd-server -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
      #       if [[ -n "$ARGOCD_SERVER" ]]; then
      #         echo "‚úÖ ArgoCD LoadBalancer: $ARGOCD_SERVER"
      #         echo "ARGOCD_SERVER=$ARGOCD_SERVER" >> $GITHUB_ENV
      #         break
      #       fi
      #       echo "‚è≥ LoadBalancer not ready yet. Retrying in 30s..."
      #       sleep 30
      #     done

      #     if [[ -z "$ARGOCD_SERVER" ]]; then
      #       echo "‚ùå ERROR: LoadBalancer hostname not assigned."
      #       exit 1
      #     fi

      # - name: Wait for DNS Resolution
      #   run: |
      #     echo "üîç Waiting for DNS resolution..."
      #     for i in {1..10}; do
      #       if nslookup $ARGOCD_SERVER; then
      #         echo "‚úÖ DNS resolved successfully!"
      #         exit 0
      #       fi
      #       echo "‚è≥ DNS not resolved yet. Retrying in 30s..."
      #       sleep 30
      #     done

      #     echo "‚ùå ERROR: DNS resolution failed for $ARGOCD_SERVER"
      #     exit 1

      # - name: Wait for LoadBalancer to Accept Connections
      #   run: |
      #     echo "üîÑ Waiting for ArgoCD LoadBalancer to accept connections..."
      #     for i in {1..10}; do
      #       if curl -k --connect-timeout 5 https://$ARGOCD_SERVER; then
      #         echo "‚úÖ LoadBalancer is accepting connections!"
      #         exit 0
      #       fi
      #       echo "‚è≥ LoadBalancer not accepting connections yet. Retrying in 30s..."
      #       sleep 30
      #     done
      #     echo "‚ùå ERROR: LoadBalancer is not accepting connections."
      #     exit 1






      - name: Log into ArgoCD (Ignoring Certificate Errors)
        run: |
          echo "üîë Attempting to log into ArgoCD..."
          for i in {1..5}; do
            if argocd login $ARGOCD_SERVER --username admin --password "password" --insecure; then
              echo "‚úÖ Successfully logged into ArgoCD"
              exit 0
            fi
            echo "‚è≥ Login failed. Retrying in 20s..."
            sleep 20
          done
          echo "‚ùå ERROR: ArgoCD login failed"
          exit 1

      - name: Wait for ArgoCD Server to Be Ready
        run: |
          echo "üîÑ Waiting for ArgoCD server to be ready..."
          for i in {1..10}; do
            if kubectl get pods -n argocd | grep -q "argocd-server.*Running"; then
              echo "‚úÖ ArgoCD server is running!"
              exit 0
            fi
            echo "‚è≥ ArgoCD server not ready yet. Retrying in 10s..."
            sleep 10
          done
          echo "‚ùå ERROR: ArgoCD server did not start in time."
          exit 1

      # - name: Start Port-Forwarding for ArgoCD
      #   run: |
      #     echo "üîÑ Starting port-forwarding to ArgoCD..."
      #     kubectl port-forward svc/argocd-server -n argocd 8085:443 > /dev/null 2>&1 & echo $! > port_forward_pid
      #     sleep 5
      #     echo "‚úÖ Port-forwarding started successfully"
    
      - name: Start Port-Forwarding for ArgoCD
        run: |
          echo "üîÑ Starting port-forwarding to ArgoCD..."
          kubectl port-forward svc/argocd-server -n argocd 8085:443 > port_forward.log 2>&1 & echo $! > port_forward_pid
          sleep 5
          if ! ps -p $(cat port_forward_pid) > /dev/null; then
            echo "‚ùå Port-forwarding process failed to start"
            exit 1
          fi
          echo "‚úÖ Port-forwarding started successfully"


      - name: Ensure ArgoCD Server is Reachable
        run: |
          echo "üîç Checking if ArgoCD is reachable..."
          for i in {1..5}; do
            if nc -z localhost 8085; then
              echo "‚úÖ ArgoCD is accessible on localhost:8085"
              exit 0
            fi
            echo "‚è≥ Waiting for ArgoCD port-forward..."
            sleep 5
          done
          echo "‚ùå ERROR: Could not connect to ArgoCD on localhost:8085"



      - name: Ensure ArgoCD CLI Recognizes 'admin'
        run: |
          echo "üîç Checking ArgoCD account permissions..."
          argocd account get --server=localhost:8085 || (echo "‚ùå 'admin' account missing, failing job" && exit 1)
          echo "‚úÖ ArgoCD account verified"
          

      - name: Ensure 'default' Project Exists
        run: |
          echo "üîß Checking if 'default' project exists..."
          if ! argocd proj get default; then
            echo "üîÑ Creating 'default' project..."
            argocd proj create default
          fi
          echo "‚úÖ 'default' project exists"

      - name: Ensure 'admin' Role Exists in Project
        run: |
          echo "üîß Checking if 'admin' role exists in 'default' project..."
          if ! argocd proj role list default | grep -q "admin"; then
            echo "üîÑ Creating 'admin' role in 'default' project..."
            argocd proj role create default admin
          fi
          echo "‚úÖ 'admin' role exists in 'default' project"





      - name: Assign Full Sync Permissions to 'admin' Role
        run: |
          echo "üîß Assigning full sync permissions to 'admin' role..."
          argocd proj role add-policy default admin -a get -o applications/* -p allow
          argocd proj role add-policy default admin -a sync -o applications/* -p allow
          argocd proj role add-policy default admin -a update -o applications/* -p allow
          argocd proj role add-policy default admin -a override -o applications/* -p allow
          argocd proj role add-policy default admin -a create -o applications/* -p allow
          argocd proj role add-policy default admin -a delete -o applications/* -p allow
          echo "‚úÖ Permissions updated"


      - name: Assign Full Project-Level Permissions to 'admin' Role
        run: |
          echo "üîß Assigning project-level permissions to 'admin' role..."
          argocd proj role add-policy default admin -a get -o projects/default -p allow
          argocd proj role add-policy default admin -a update -o projects/default -p allow
          argocd proj role add-policy default admin -a sync -o projects/default -p allow
          argocd proj role add-policy default admin -a override -o projects/default -p allow
          echo "‚úÖ Project-level permissions assigned"

      - name: Assign Application-Level Access to 'admin' Role
        run: |
          echo "üîß Assigning explicit permissions for 'admin' role on 'demo-app'..."
          argocd proj role add-policy default admin -a get -o applications/demo-app -p allow
          argocd proj role add-policy default admin -a update -o applications/demo-app -p allow
          argocd proj role add-policy default admin -a sync -o applications/demo-app -p allow
  
       

          echo "‚úÖ 'admin' role permissions for 'demo-app' updated"


      - name: Assign Namespace & Cluster-Level Access
        run: |
          echo "üîß Assigning namespace and cluster-level access to 'admin' role..."
          argocd proj allow-cluster-resource default "*" "*"
          argocd proj allow-namespace-resource default "*" "*"
          echo "‚úÖ Namespace and cluster-wide access granted"



      
      - name: Ensure Global ArgoCD Authorization for 'admin'
        run: |
          echo "üîß Ensuring 'admin' is globally authorized in ArgoCD..."
          kubectl patch configmap argocd-cm -n argocd --type merge -p \
            '{"data":{"policy.default":"role:admin"}}'
          echo "‚úÖ Global authorization applied"
          
      - name: üîß Update ArgoCD RBAC settings
        run: |
          echo "üîß Updating ArgoCD RBAC settings..."
      
          kubectl patch configmap argocd-rbac-cm -n argocd --type merge -p \
          '{"data": {"policy.csv": "g, my-group, role:admin\ng, dev-team, role:readonly"}}'
      
          echo "‚úÖ ArgoCD RBAC settings updated successfully!"


          
      - name: Ensure ArgoCD Controller Has Full Cluster Access
        run: |
          echo "üîß Granting cluster-admin access to ArgoCD controller..."
          kubectl create clusterrolebinding argocd-admin-binding --clusterrole=cluster-admin --serviceaccount=argocd:argocd-application-controller || true
          echo "‚úÖ ArgoCD controller has cluster-admin access"

          


      
      - name: Restart ArgoCD Server to Apply Authorization Changes
        run: |
          echo "üîÑ Restarting ArgoCD to apply changes..."
          kubectl rollout restart deployment argocd-server -n argocd
          sleep 30
          echo "‚úÖ Restart complete"

      - name: Log into ArgoCD (Ignoring Certificate Errors)
        run: |
          echo "üîë Attempting to log into ArgoCD..."
          for i in {1..5}; do
            if argocd login $ARGOCD_SERVER --username admin --password "password" --insecure; then
              echo "‚úÖ Successfully logged into ArgoCD"
              exit 0
            fi
            echo "‚è≥ Login failed. Retrying in 20s..."
            sleep 20
          done
          echo "‚ùå ERROR: ArgoCD login failed"
          exit 1

      - name: Wait for ArgoCD Server to Be Ready
        run: |
          echo "üîÑ Waiting for ArgoCD server pods to be ready..."
          for i in {1..10}; do
            if kubectl get pods -n argocd | grep -q "argocd-server.*Running"; then
              echo "‚úÖ ArgoCD server is running!"
              exit 0
            fi
            echo "‚è≥ ArgoCD server not ready yet. Retrying in 10s..."
            sleep 10
          done
          echo "‚ùå ERROR: ArgoCD server did not start in time."
          exit 1

      - name: Start Port-Forwarding for ArgoCD
        run: |
          echo "üîÑ Checking if another process is using port 8085..."
          kill -9 $(lsof -t -i :8085) 2>/dev/null || true
          
          echo "üîÑ Checking available ports on argocd-server..."
          kubectl get svc argocd-server -n argocd -o yaml | grep "port:"

          echo "üîÑ Starting port-forwarding to ArgoCD..."
          nohup kubectl port-forward svc/argocd-server -n argocd 8085:443 > port_forward.log 2>&1 & echo $! > port_forward_pid
          sleep 5
          
          if ! ps -p $(cat port_forward_pid) > /dev/null; then
            echo "‚ùå Port-forwarding process failed to start"
            exit 1
          fi
          
          echo "‚úÖ Port-forwarding started successfully"

      - name: Ensure ArgoCD Server is Reachable
        run: |
          echo "üîç Checking if ArgoCD is reachable..."
          for i in {1..5}; do
            if nc -z localhost 8085; then
              echo "‚úÖ ArgoCD is accessible on localhost:8085"
              exit 0
            fi
            echo "‚è≥ Waiting for ArgoCD port-forward..."
            sleep 5
          done
          echo "‚ùå ERROR: Could not connect to ArgoCD on localhost:8085"
          exit 1


      - name: Verify Port-Forwarding is Still Active
        run: |
          echo "üîç Checking if port-forwarding is still active..."
          if ! ps -p $(cat port_forward_pid) > /dev/null; then
            echo "‚ö†Ô∏è Port-forwarding process is missing. Restarting..."
            kubectl port-forward svc/argocd-server -n argocd 8085:443 > port_forward.log 2>&1 & echo $! > port_forward_pid
            sleep 5
          fi
          echo "‚úÖ Port-forwarding process is running"

      - name: Debug - Verify 'admin' Role Permissions in ArgoCD
        run: |
          echo "üîç Checking 'admin' role permissions in ArgoCD..."
          argocd proj role get default admin --server=localhost:8085 || (echo "‚ùå 'admin' role missing or has incorrect permissions!" && exit 1)
          echo "‚úÖ 'admin' role permissions verified"

      - name: üîç Checking if 'demo-app' exists...
        run: argocd app list

              
      - name: Recreate ArgoCD Application
        run: |
          echo "üöÄ Recreating ArgoCD application..."
          kubectl delete -f argocd/application.yaml --ignore-not-found
          sed -i "s|AWS_ACCOUNT_ID|${{ secrets.AWS_ACCOUNT_ID }}|g" argocd/application.yaml
          kubectl apply -f argocd/application.yaml

          kubectl apply -f argocd/application.yaml

      - name: üîç Checking if 'demo-app' exists222..
        run: argocd app list    

          
      - name: Debug - Check ArgoCD Logs for RBAC Issues
        run: |
          echo "üîç Checking ArgoCD logs for RBAC rejections..."
          kubectl logs -n argocd -l app.kubernetes.io/name=argocd-server | grep "RBAC" || echo "‚úÖ No RBAC rejections found"
    
      - name: Debug - List Applications to Check Ownership
        run: |
          echo "üîç Listing ArgoCD applications..."
          argocd app list --server=localhost:8085
          echo "‚úÖ ArgoCD applications listed"
          
      - name: Debug - Verify 'demo-app' Exists Before Assignment
        run: |
          echo "üîç Checking if 'demo-app' exists..."
          if ! argocd app get demo-app --server=localhost:8085; then
            echo "‚ùå 'demo-app' does not exist or is inaccessible."
            exit 1
          fi
          echo "‚úÖ 'demo-app' exists and is accessible."



      - name: Debug - Check If 'admin' Can Access 'demo-app'
        run: |
          echo "üîç Checking if 'admin' can get 'demo-app'..."
          if ! argocd app get demo-app --server=localhost:8085; then
            echo "‚ùå 'admin' cannot access 'demo-app'. This is likely an RBAC issue!"
            exit 1
          fi
          echo "‚úÖ 'admin' has access to 'demo-app'"

          
      - name: Ensure 'demo-app' Is Assigned to 'default' Project
        run: |
          echo "üîç Checking if 'admin' owns 'demo-app'..."
          argocd app set demo-app --project default --server=localhost:8085
          echo "‚úÖ 'demo-app' ownership reassigned to 'default' proj



      - name: Ensure 'demo-app' Ownership
        run: |
          echo "üîç Checking if 'admin' owns 'demo-app'..."
          argocd app get demo-app --server=localhost:8085 || \
          argocd app set demo-app --project default --server=localhost:8085
          echo "‚úÖ Ownership verified"

    
      - name: Verify 'admin' Role Permissions
        run: |
          echo "üîç Verifying 'admin' role permissions..."
          argocd proj role get default admin

      - name: Sync ArgoCD Application
        run: |
          echo "üöÄ Syncing demo-app with ArgoCD..."
          argocd app sync demo-app --server=$ARGOCD_SERVER
          echo "‚úÖ Sync initiated successfully"

      - name: Port Forward as Fallback
        if: failure()
        run: |
          echo "‚ö†Ô∏è Using port-forwarding as a fallback..."
          kubectl port-forward svc/argocd-server -n argocd 8085:443 > /dev/null 2>&1 & echo $! > port_forward_pid
          sleep 5

          echo "üîë Logging into ArgoCD via port-forward..."
          argocd login localhost:8085 --username admin --password "password" --insecure
          echo "‚úÖ Successfully logged in using port-forwarding"

          echo "üöÄ Syncing demo-app with ArgoCD via port-forward..."
          argocd app sync demo-app --server=localhost:8085 || (echo "‚ùå Sync failed via port-forwarding." && exit 1)
          echo "‚úÖ Sync initiated successfully (fallback mode)"

          echo "üõë Cleaning up port-forwarding..."
          kill $(cat port_forward_pid)


    
